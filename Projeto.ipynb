{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6186090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import musdb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import gc\n",
    "import time\n",
    "import IPython.display as ipd\n",
    "import random \n",
    "import museval\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57761f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação do banco de dados e pré-processamento\n",
    "#RODAR SÓ SE NÃO TIVER SALVO\n",
    "mus = musdb.DB(root=\"/Users/gusta/MUSDB18\")\n",
    "#Stereo\n",
    "# 0-99 Train set\n",
    "# 100-149 Test set\n",
    "# 0 - The mixture,\n",
    "# 1 - The drums,\n",
    "# 2 - The bass,\n",
    "# 3 - The rest of the accompaniment,\n",
    "# 4 - The vocals.\n",
    "\n",
    "#Conversão de stereo para mono e reamostragem\n",
    "sr = 16000\n",
    "i = 0 \n",
    "\n",
    "for tracks in mus:\n",
    "    aux = librosa.to_mono(tracks.stems[0,:,:].T)\n",
    "    aux = librosa.resample(aux, orig_sr = 44100, target_sr = sr, res_type='kaiser_best', fix=True, scale=False)\n",
    "    \n",
    "    aux_per = librosa.to_mono(tracks.stems[1,:,:].T)\n",
    "    aux_per = librosa.resample(aux_per, orig_sr = 44100, target_sr = sr, res_type='kaiser_best', fix=True, scale=False)\n",
    "    \n",
    "    aux_har = librosa.to_mono(tracks.stems[3,:,:].T)\n",
    "    aux_har = librosa.resample(aux_har, orig_sr = 44100, target_sr = sr, res_type='kaiser_best', fix=True, scale=False)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(aux)\n",
    "    df.to_csv('C:/Users/gusta\\Misturas/'+'Mixture_'+str(i)+'.csv', index = False)\n",
    "    \n",
    "    df = pd.DataFrame(aux_per)\n",
    "    df.to_csv('C:/Users/gusta\\Percurssao/'+'Percurssao_'+str(i)+'.csv', index = False)\n",
    "    \n",
    "    df = pd.DataFrame(aux_har)\n",
    "    df.to_csv('C:/Users/gusta\\Harmonico/'+'Harmonico_'+str(i)+'.csv', index = False)\n",
    "    \n",
    "    i = i + 1\n",
    "    if i%15 == 0:\n",
    "        print(str(i/1.5)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funções\n",
    "\n",
    "#Function that shows specifc Spectrogram\n",
    "def stem_espectrograma(music_index, stem, sr, hop_length, n_window, n_fft):\n",
    "    mus = musdb.DB(root=\"/Users/gusta/MUSDB18\")\n",
    "    sr = 16000\n",
    "    i = 0 \n",
    "\n",
    "    aux = librosa.to_mono(mus.tracks[int(music_index)].stems[int(stem),:,:].T)\n",
    "    aux = librosa.resample(aux, orig_sr = 44100, target_sr = sr, res_type='kaiser_best', fix=True, scale=False)\n",
    "\n",
    "    X = librosa.stft(aux, win_length = n_window, n_fft = n_fft, hop_length = hop_length, window = 'hamming')\n",
    "    D = librosa.amplitude_to_db(np.abs(X), ref=np.max)\n",
    "    plt.figure(figsize=(20, 7))\n",
    "    librosa.display.specshow(D, y_axis = 'linear', x_axis='time', sr = 16000,\n",
    "    hop_length=hop_length) # , cmap='gray_r'\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Linear-frequency power spectrogram')\n",
    "    \n",
    "    \n",
    "#Function that shows specifc Spectrogram\n",
    "def mostra_espectrograma(tipo_track, indice_musica, hop_length, n_window, n_fft):\n",
    "    if tipo_track == 0:\n",
    "        aux = pd.read_csv('C:/Users/gusta\\Misturas/'+'Mixture_'+str(indice_musica)+'.csv')\n",
    "        aux = aux.to_numpy()\n",
    "        aux = aux.T\n",
    "    elif tipo_track == 1:\n",
    "        aux = pd.read_csv('C:/Users/gusta\\Percurssao/'+'Percurssao_'+str(indice_musica)+'.csv')\n",
    "        aux = aux.to_numpy()\n",
    "        aux = aux.T\n",
    "    else:\n",
    "        aux = pd.read_csv('C:/Users/gusta\\Harmonico/'+'Harmonico_'+str(indice_musica)+'.csv')\n",
    "        aux = aux.to_numpy()\n",
    "        aux = aux.T\n",
    "\n",
    "    X = librosa.stft(aux[0], win_length = n_window, n_fft = n_fft, hop_length = hop_length, window = 'hamming')\n",
    "    D = librosa.amplitude_to_db(np.abs(X), ref=np.max)\n",
    "    plt.figure(figsize=(20, 7))\n",
    "    librosa.display.specshow(D, y_axis = 'linear', x_axis='time', sr = 16000,\n",
    "    hop_length=hop_length) # , cmap='gray_r'\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Linear-frequency power spectrogram')\n",
    "    \n",
    "#Function that returns specifc Spectrogram\n",
    "def fornece_espectrograma(tipo_track, indice_musica, hop_length, n_window, n_fft):\n",
    "    #start_carregamento = time.time()\n",
    "    if tipo_track == 0:\n",
    "        aux = pd.read_csv('C:/Users/gusta\\Misturas/'+'Mixture_'+str(indice_musica)+'.csv')\n",
    "        aux = aux.to_numpy()\n",
    "        aux = aux.T\n",
    "    elif tipo_track == 1:\n",
    "        aux = pd.read_csv('C:/Users/gusta\\Percurssao/'+'Percurssao_'+str(indice_musica)+'.csv')\n",
    "        aux = aux.to_numpy()\n",
    "        aux = aux.T\n",
    "    else:\n",
    "        aux = pd.read_csv('C:/Users/gusta\\Harmonico/'+'Harmonico_'+str(indice_musica)+'.csv')\n",
    "        aux = aux.to_numpy()\n",
    "        aux = aux.T\n",
    "    #stop_carregamento = time.time()    \n",
    "    #start_stft = time.time()\n",
    "    X = librosa.stft(aux[0], win_length = n_window, n_fft = n_fft, hop_length = hop_length, window = 'hamming')\n",
    "    #stop_stft = time.time()\n",
    "    \n",
    "    #print (f'Carregamento [{stop_carregamento-start_carregamento}s], STFT [{stop_stft-start_stft}s]')\n",
    "\n",
    "    return X\n",
    "\n",
    "#Storing Spectrogram as 32bit float \n",
    "def prepara_dataset(hop_length, n_window, n_fft): \n",
    "    for i in range(150):\n",
    "        aux = pd.read_csv('C:/Users/gusta/Misturas/'+'Mixture_'+str(i)+'.csv')\n",
    "        aux = aux.to_numpy()\n",
    "        aux = aux.T\n",
    "        aux = abs(librosa.stft(aux[0], win_length = n_window, n_fft = n_fft, hop_length = hop_length, window = 'hamming', center=False)).astype('float32')\n",
    "        df = pd.DataFrame(aux)\n",
    "        df.to_csv('C:/Users/gusta/Misturas/Espectros/'+'Mixture_'+str(i)+'.csv', index = False)\n",
    "\n",
    "        aux = pd.read_csv('C:/Users/gusta/Percurssao/'+'Percurssao_'+str(i)+'.csv')\n",
    "        aux = aux.to_numpy()\n",
    "        aux = aux.T\n",
    "        aux = abs(librosa.stft(aux[0], win_length = n_window, n_fft = n_fft, hop_length = hop_length, window = 'hamming', center=False)).astype('float32')        \n",
    "        df = pd.DataFrame(aux)\n",
    "        df.to_csv('C:/Users/gusta/Percurssao/Espectros/'+'Percurssao_'+str(i)+'.csv', index = False)\n",
    "\n",
    "        aux = pd.read_csv('C:/Users/gusta/Harmonico/'+'Harmonico_'+str(i)+'.csv')\n",
    "        aux = aux.to_numpy()\n",
    "        aux = aux.T\n",
    "        aux = abs(librosa.stft(aux[0], win_length = n_window, n_fft = n_fft, hop_length = hop_length, window = 'hamming', center=False)).astype('float32')        \n",
    "        df = pd.DataFrame(aux)\n",
    "        df.to_csv('C:/Users/gusta/Harmonico/Espectros/'+'Harmonico_'+str(i)+'.csv', index = False)\n",
    "        i = i + 1\n",
    "        print(i)\n",
    "        if i%15 == 0:\n",
    "            print(str(i/1.5)+'%')\n",
    "\n",
    "#Load Spectrogram in Tensors \n",
    "def carrega_dataset(dataset_type, n_tracks, hop_length, n_window, n_fft): \n",
    "    size_bytes = 0\n",
    "    esp_sizes = np.zeros(n_tracks)\n",
    "    esp_hops = np.zeros(n_tracks)\n",
    "    \n",
    "    if dataset_type == 'train':\n",
    "        for i in range(n_tracks):\n",
    "            x_mix = pd.read_csv('C:/Users/gusta/Misturas/Espectros/'+'Mixture_'+str(i)+'.csv')\n",
    "            x_mix = x_mix.to_numpy()\n",
    "            size_bytes = size_bytes + x_mix.nbytes\n",
    "            esp_sizes[i] = x_mix.shape[1]\n",
    "            if i > 0:\n",
    "               esp_hops[i] = esp_hops[i-1]+esp_sizes[i-1]\n",
    "        print(\"Dataset: \"+str(dataset_type)+\" Tamanho estimado: \"+str(3*size_bytes/(1024**3))+\" GB\")\n",
    "\n",
    "        dataset_mix = torch.zeros(int(np.floor((n_fft/2 +1))),int(esp_hops[n_tracks-1]+esp_sizes[n_tracks-1]))\n",
    "        dataset_per = torch.zeros(int(np.floor((n_fft/2 +1))),int(esp_hops[n_tracks-1]+esp_sizes[n_tracks-1]))\n",
    "        dataset_har = torch.zeros(int(np.floor((n_fft/2 +1))),int(esp_hops[n_tracks-1]+esp_sizes[n_tracks-1]))\n",
    "\n",
    "        for i in range(n_tracks):\n",
    "            esp_mix = pd.read_csv('C:/Users/gusta/Misturas/Espectros/'+'Mixture_'+str(i)+'.csv') \n",
    "            esp_mix = esp_mix.to_numpy()\n",
    "\n",
    "            esp_per = pd.read_csv('C:/Users/gusta/Percurssao/Espectros/'+'Percurssao_'+str(i)+'.csv') \n",
    "            esp_per = esp_per.to_numpy()\n",
    "\n",
    "            esp_har = pd.read_csv('C:/Users/gusta/Harmonico/Espectros/'+'Harmonico_'+str(i)+'.csv') \n",
    "            esp_har = esp_har.to_numpy()\n",
    "\n",
    "            dataset_mix[:,int(esp_hops[i]):int(esp_hops[i]+esp_sizes[i])] = torch.from_numpy(esp_mix)\n",
    "            dataset_per[:,int(esp_hops[i]):int(esp_hops[i]+esp_sizes[i])] = torch.from_numpy(esp_per)\n",
    "            dataset_har[:,int(esp_hops[i]):int(esp_hops[i]+esp_sizes[i])] = torch.from_numpy(esp_har)\n",
    "\n",
    "            if ((i+1)%(n_tracks/10))%10 == 0:\n",
    "                print(str(((i+1)*100)/n_tracks)+'%')\n",
    "    else:\n",
    "        for i in range(n_tracks):\n",
    "            x_mix = pd.read_csv('C:/Users/gusta/Misturas/Espectros/'+'Mixture_'+str(i+100)+'.csv')\n",
    "            x_mix = x_mix.to_numpy()\n",
    "            size_bytes = size_bytes + x_mix.nbytes\n",
    "            esp_sizes[i] = x_mix.shape[1]\n",
    "            if i > 0:\n",
    "               esp_hops[i] = esp_hops[i-1]+esp_sizes[i-1]\n",
    "        print(\"Dataset: \"+str(dataset_type)+\" Tamanho estimado: \"+str(3*size_bytes/(1024**3))+\" GB\")\n",
    "\n",
    "        dataset_mix = torch.zeros(int(np.floor((n_fft/2 +1))),int(esp_hops[n_tracks-1]+esp_sizes[n_tracks-1]))\n",
    "        dataset_per = torch.zeros(int(np.floor((n_fft/2 +1))),int(esp_hops[n_tracks-1]+esp_sizes[n_tracks-1]))\n",
    "        dataset_har = torch.zeros(int(np.floor((n_fft/2 +1))),int(esp_hops[n_tracks-1]+esp_sizes[n_tracks-1]))\n",
    "\n",
    "        for i in range(n_tracks):\n",
    "            esp_mix = pd.read_csv('C:/Users/gusta/Misturas/Espectros/'+'Mixture_'+str(i+100)+'.csv') \n",
    "            esp_mix = esp_mix.to_numpy()\n",
    "\n",
    "            esp_per = pd.read_csv('C:/Users/gusta/Percurssao/Espectros/'+'Percurssao_'+str(i+100)+'.csv') \n",
    "            esp_per = esp_per.to_numpy()\n",
    "\n",
    "            esp_har = pd.read_csv('C:/Users/gusta/Harmonico/Espectros/'+'Harmonico_'+str(i+100)+'.csv') \n",
    "            esp_har = esp_har.to_numpy()\n",
    "\n",
    "            dataset_mix[:,int(esp_hops[i]):int(esp_hops[i]+esp_sizes[i])] = torch.from_numpy(esp_mix)\n",
    "            dataset_per[:,int(esp_hops[i]):int(esp_hops[i]+esp_sizes[i])] = torch.from_numpy(esp_per)\n",
    "            dataset_har[:,int(esp_hops[i]):int(esp_hops[i]+esp_sizes[i])] = torch.from_numpy(esp_har)\n",
    "\n",
    "            if ((i+1)%(n_tracks/10))%10 == 0:\n",
    "                print(str(((i+1)*100)/n_tracks)+'%')\n",
    "    return esp_sizes, esp_hops, dataset_mix, dataset_per, dataset_har\n",
    "\n",
    "#Function that returns total number of batches in the dataset\n",
    "def total_batches(dataset_type, n_tracks, batch_size, size, stride):\n",
    "    n_batches = 0\n",
    "    \n",
    "    if dataset_type == 'train':\n",
    "        print(\"Dataset: \"+str(dataset_type))\n",
    "        for i in range(n_tracks):\n",
    "            x_mix = pd.read_csv('C:/Users/gusta/Misturas/Espectros/'+'Mixture_'+str(i)+'.csv') \n",
    "            x_mix = x_mix.to_numpy()\n",
    "            n_slices_vertical = np.floor((x_mix.shape[0] - size)/stride + 1)\n",
    "            n_slices_horizontal = np.floor((x_mix.shape[1] - size)/stride + 1)\n",
    "            n_slices_total = n_slices_vertical*n_slices_horizontal\n",
    "            n_batches = n_batches + n_slices_total\n",
    "            i = i +1\n",
    "            if (i%(n_tracks/10))%10 == 0:\n",
    "                print(str((i*100)/n_tracks)+'%')\n",
    "        print(\"Banco de dados contém: \")\n",
    "        print(\"Número de slices totais: \"+str(n_batches)+\", Número de Batches: \"+str(np.floor(n_batches/batch_size))+\", Tamanho: \"+str(batch_size))\n",
    "        print(\"Número de Slices restantes: \"+str(n_batches%batch_size))\n",
    "        return np.floor(n_batches/batch_size)\n",
    "    else:\n",
    "        print(\"Dataset: \"+str(dataset_type))\n",
    "        for i in range(n_tracks):\n",
    "            x_mix = pd.read_csv('C:/Users/gusta/Misturas/Espectros/'+'Mixture_'+str(i+100)+'.csv') \n",
    "            x_mix = x_mix.to_numpy()\n",
    "            n_slices_vertical = np.floor((x_mix.shape[0] - size)/stride + 1)\n",
    "            n_slices_horizontal = np.floor((x_mix.shape[1] - size)/stride + 1)\n",
    "            n_slices_total = n_slices_vertical*n_slices_horizontal\n",
    "            n_batches = n_batches + n_slices_total\n",
    "            i = i +1\n",
    "            if (i%(n_tracks/10))%10 == 0:\n",
    "                print(str((i*100)/n_tracks)+'%')\n",
    "        print(\"Banco de dados contém: \")\n",
    "        print(\"Número de slices totais: \"+str(n_batches)+\", Número de Batches: \"+str(np.floor(n_batches/batch_size))+\", Tamanho: \"+str(batch_size))\n",
    "        print(\"Número de Slices restantes: \"+str(n_batches%batch_size))\n",
    "        return np.floor(n_batches/batch_size)\n",
    "    \n",
    "#Function that returns batches (squares) for training\n",
    "def fornece_batches(esp_sizes, esp_hops, dataset_mix, dataset_per, dataset_har, batch_size, current_track, slices_left, selection, size, stride, n_fft, normalization):\n",
    "    if current_track == -1:\n",
    "        current_track = selection[0]\n",
    "        slices_left = 0\n",
    "    \n",
    "    n_slices_vertical = np.floor(((np.floor(n_fft/2 + 1) - size)/stride) + 1)    \n",
    "    \n",
    "    batch_mix = torch.zeros(int(batch_size),1,int(size),int(size))\n",
    "    batch_per = torch.zeros(int(batch_size),1,int(size),int(size))\n",
    "    batch_har = torch.zeros(int(batch_size),1,int(size),int(size))\n",
    "    \n",
    "    #stop_preparing = time.time()\n",
    "    \n",
    "    batch_index = 0\n",
    "    while batch_index < batch_size:\n",
    "        #start_loading = time.time()\n",
    "        \n",
    "        n_slices_horizontal = np.floor((esp_sizes[current_track] - size)/stride + 1)\n",
    "        n_slices_total = n_slices_vertical*n_slices_horizontal\n",
    "\n",
    "        if slices_left == 0: \n",
    "            slices_left = n_slices_total\n",
    "\n",
    "        if slices_left > (batch_size-batch_index): \n",
    "            current_track_batch = batch_size - batch_index\n",
    "        else: \n",
    "            current_track_batch = slices_left\n",
    "\n",
    "        i = np.floor((n_slices_total-slices_left)/n_slices_vertical)\n",
    "        j = (n_slices_total-slices_left)%n_slices_vertical\n",
    "\n",
    "        slices_left = slices_left - current_track_batch\n",
    "\n",
    "        #stop_definitions = time.time()\n",
    "\n",
    "        #start_while = time.time()\n",
    "        if normalization == True:\n",
    "            while current_track_batch > 0:\n",
    "                #print('Tamanho do espectrograma: '+str(dataset_mix.shape))\n",
    "                #print('Limites Verticais: ('+str(j*stride)+\", \"+str(j*stride+size)+') , Limites Horizontais: ('+str(esp_hops[int(current_track)]+i*stride)+\", \"+str(esp_hops[int(current_track)]+i*stride+size)+\")\")\n",
    "\n",
    "                batch_mix[int(batch_index),0,:,:] = dataset_mix[int(j*stride):int(j*stride+size),int(esp_hops[int(current_track)]+i*stride):int(esp_hops[int(current_track)]+i*stride+size)]\n",
    "                #print(\"Mix Mean\",batch_mix[int(batch_index),0,:,:].mean(),\"Mix Var\",batch_mix[int(batch_index),0,:,:].var())\n",
    "                batch_mix[int(batch_index),0,:,:] = (batch_mix[int(batch_index),0,:,:]-batch_mix[int(batch_index),0,:,:].mean())/(torch.sqrt(batch_mix[int(batch_index),0,:,:].var()+1e-16))\n",
    "                batch_per[int(batch_index),0,:,:] = dataset_per[int(j*stride):int(j*stride+size),int(esp_hops[int(current_track)]+i*stride):int(esp_hops[int(current_track)]+i*stride+size)] \n",
    "                #print(\"Per Mean\",batch_per[int(batch_index),0,:,:].mean(),\"Per Var\",batch_per[int(batch_index),0,:,:].var())\n",
    "                batch_per[int(batch_index),0,:,:] = (batch_per[int(batch_index),0,:,:]-batch_per[int(batch_index),0,:,:].mean())/(torch.sqrt(batch_per[int(batch_index),0,:,:].var()+1e-16))\n",
    "                batch_har[int(batch_index),0,:,:] = dataset_har[int(j*stride):int(j*stride+size),int(esp_hops[int(current_track)]+i*stride):int(esp_hops[int(current_track)]+i*stride+size)]\n",
    "                #print(\"Har Mean\",batch_har[int(batch_index),0,:,:].mean(),\"Har Var\",batch_har[int(batch_index),0,:,:].var())\n",
    "                batch_har[int(batch_index),0,:,:] = (batch_har[int(batch_index),0,:,:]-batch_har[int(batch_index),0,:,:].mean())/(torch.sqrt(batch_har[int(batch_index),0,:,:].var()+1e-16))\n",
    "                \n",
    "                j = (j+1)%n_slices_vertical\n",
    "                if j == 0: i = i + 1\n",
    "                batch_index = batch_index + 1\n",
    "                current_track_batch = current_track_batch - 1\n",
    "        else:\n",
    "            while current_track_batch > 0:\n",
    "                    #print('Tamanho do espectrograma: '+str(dataset_mix.shape))\n",
    "                    #print('Limites Verticais: ('+str(j*stride)+\", \"+str(j*stride+size)+') , Limites Horizontais: ('+str(esp_hops[int(current_track)]+i*stride)+\", \"+str(esp_hops[int(current_track)]+i*stride+size)+\")\")\n",
    "\n",
    "                    batch_mix[int(batch_index),0,:,:] = dataset_mix[int(j*stride):int(j*stride+size),int(esp_hops[int(current_track)]+i*stride):int(esp_hops[int(current_track)]+i*stride+size)]\n",
    "                    batch_per[int(batch_index),0,:,:] = dataset_per[int(j*stride):int(j*stride+size),int(esp_hops[int(current_track)]+i*stride):int(esp_hops[int(current_track)]+i*stride+size)] \n",
    "                    batch_har[int(batch_index),0,:,:] = dataset_har[int(j*stride):int(j*stride+size),int(esp_hops[int(current_track)]+i*stride):int(esp_hops[int(current_track)]+i*stride+size)]\n",
    "\n",
    "                    j = (j+1)%n_slices_vertical\n",
    "                    if j == 0: i = i + 1\n",
    "                    batch_index = batch_index + 1\n",
    "                    current_track_batch = current_track_batch - 1\n",
    "\n",
    "        #print(\"Track:\"+str(current_track)+\" Faltantes:\"+str(slices_left)+\" Slices Totais:\"+str(n_slices_total)+\" Batch Size\"+\"(\"+str(batch_index)+\")\")           \n",
    "        if slices_left == 0: \n",
    "            current_track = selection[(np.where(selection==int(current_track)))[0].item()+1]\n",
    "            \n",
    "        #print('----------------------------')   \n",
    "        \n",
    "    return current_track, slices_left, batch_mix, batch_per, batch_har \n",
    "\n",
    "#Apply model to obtain stems\n",
    "def apply_model(indice_musica, model, batch_size, n_window, n_fft, hop_length, size, stride, normalization, pct_limiar): \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        track = pd.read_csv('C:/Users/gusta/Misturas/'+'Mixture_'+str(indice_musica)+'.csv')\n",
    "        track = track.to_numpy()\n",
    "        track = track.T\n",
    "\n",
    "        track_per = pd.read_csv('C:/Users/gusta/Percurssao/'+'Percurssao_'+str(indice_musica)+'.csv')\n",
    "        track_per = track_per.to_numpy()\n",
    "        track_per = track_per.T\n",
    "\n",
    "        track_har = pd.read_csv('C:/Users/gusta/Harmonico/'+'Harmonico_'+str(indice_musica)+'.csv')\n",
    "        track_har = track_har.to_numpy()\n",
    "        track_har = track_har.T\n",
    "\n",
    "        esp = (librosa.stft(track[0], win_length = n_window, n_fft = n_fft, hop_length = hop_length, window = 'hamming', center=False))       \n",
    "        \n",
    "        x_mix = pd.read_csv('C:/Users/gusta/Misturas/Espectros/'+'Mixture_'+str(indice_musica)+'.csv')\n",
    "        x_mix = x_mix.to_numpy()\n",
    "\n",
    "        n_slices_vertical = np.floor(((np.floor(n_fft/2 + 1) - size)/stride) + 1)  \n",
    "        n_slices_horizontal = np.floor((x_mix.shape[1] - size)/stride + 1)\n",
    "        n_slices_total = n_slices_vertical*n_slices_horizontal\n",
    "        \n",
    "        #print(\"Dim_Vertical\", x_mix.shape[0], \"Dim_Horizontal\", x_mix.shape[1])\n",
    "        sobra_vertical = x_mix.shape[0]-(size+(n_slices_vertical-1)*stride)\n",
    "        sobra_horizontal = x_mix.shape[1]-(size+(n_slices_horizontal-1)*stride)\n",
    "        #print(\"Sobra_Vertical\", sobra_vertical, \"Sobra_Horizontal\", sobra_horizontal)\n",
    "        \n",
    "        if sobra_vertical > 0:\n",
    "            enxerto_vertical = np.zeros((int(sobra_vertical),int(x_mix.shape[1])))\n",
    "            enxerto_vertical = esp[int(x_mix.shape[0]-sobra_vertical):int(x_mix.shape[0]),0:int(x_mix.shape[1]-sobra_horizontal)]\n",
    "\n",
    "        phase = np.angle(esp[0:int(x_mix.shape[0]-sobra_vertical),0:int(x_mix.shape[1]-sobra_horizontal)])\n",
    "        \n",
    "        x_per = np.zeros((int(x_mix.shape[0]-sobra_vertical),int(x_mix.shape[1]-sobra_horizontal)))\n",
    "        x_har = np.zeros((int(x_mix.shape[0]-sobra_vertical),int(x_mix.shape[1]-sobra_horizontal)))\n",
    "        batch_mix = torch.zeros(int(n_slices_total),1,int(size),int(size))\n",
    "        batch_per = torch.zeros(int(n_slices_total),1,int(size),int(size))\n",
    "        batch_har = torch.zeros(int(n_slices_total),1,int(size),int(size))\n",
    "\n",
    "        x = 0\n",
    "        y = 0\n",
    "        batch_index = 0\n",
    "\n",
    "        while batch_index < n_slices_total:\n",
    "                batch_mix[int(batch_index),0,:,:] = torch.from_numpy(x_mix[int(y*stride):int(y*stride+size),int(x*stride):int(x*stride+size)])\n",
    "                if normalization == True:\n",
    "                    batch_mix[int(batch_index),0,:,:] = (batch_mix[int(batch_index),0,:,:]-batch_mix[int(batch_index),0,:,:].mean())/(torch.sqrt(batch_mix[int(batch_index),0,:,:].var()+1e-16))\n",
    "                \n",
    "                y = (y+1)%n_slices_vertical\n",
    "                if y == 0: x = x + 1\n",
    "                batch_index = batch_index + 1\n",
    "\n",
    "        x = np.floor(n_slices_total/batch_size)\n",
    "        y = np.floor(n_slices_total%batch_size)\n",
    "        batch_index = 0\n",
    "        \n",
    "        batch_mix = batch_mix.to(device)\n",
    "        for batch_index in range(int(x)):\n",
    "            batch_per[int(batch_index*batch_size):int(batch_index*batch_size+batch_size),:,:,:], batch_har[int(batch_index*batch_size):int(batch_index*batch_size+batch_size),:,:,:] = model(batch_mix[int(batch_index*batch_size):int(batch_index*batch_size+batch_size),:,:,:])\n",
    "        \n",
    "        batch_per[int(x*batch_size):int(x*batch_size+y),:,:,:], batch_har[int(x*batch_size):int(x*batch_size+y),:,:,:] = model(batch_mix[int(x*batch_size):int(x*batch_size+y),:,:,:])\n",
    "        \n",
    "        batch_per = batch_per.to('cpu')\n",
    "        batch_har = batch_har.to('cpu')\n",
    "        \n",
    "\n",
    "        \n",
    "        x = 0\n",
    "        y = 0\n",
    "        batch_index = 0\n",
    "\n",
    "        while batch_index < n_slices_total:\n",
    "            if batch_index < n_slices_vertical:\n",
    "                if y == 0:\n",
    "                    x_per[int(y*stride):int(y*stride+size),int(x*stride):int(x*stride+size)] = batch_per[int(batch_index),0,:,:]\n",
    "                    x_har[int(y*stride):int(y*stride+size),int(x*stride):int(x*stride+size)] = batch_har[int(batch_index),0,:,:]\n",
    "                else:\n",
    "                    x_per[int(y*stride+size-stride):int(y*stride+size),int(x*stride):int(x*stride+size)] = batch_per[int(batch_index),0,int(size-stride):int(size),:]\n",
    "                    x_har[int(y*stride+size-stride):int(y*stride+size),int(x*stride):int(x*stride+size)] = batch_har[int(batch_index),0,int(size-stride):int(size),:]\n",
    "            else:\n",
    "                if y == 0:\n",
    "                    x_per[int(y*stride):int(y*stride+size),int(x*stride+size-stride):int(x*stride+size)] = batch_per[int(batch_index),0,:,int(size-stride):int(size)]\n",
    "                    x_har[int(y*stride):int(y*stride+size),int(x*stride+size-stride):int(x*stride+size)] = batch_har[int(batch_index),0,:,int(size-stride):int(size)]\n",
    "                else:\n",
    "                    x_per[int(y*stride+size-stride):int(y*stride+size),int(x*stride+size-stride):int(x*stride+size)] = batch_per[int(batch_index),0,int(size-stride):int(size),int(size-stride):int(size)]\n",
    "                    x_har[int(y*stride+size-stride):int(y*stride+size),int(x*stride+size-stride):int(x*stride+size)] = batch_har[int(batch_index),0,int(size-stride):int(size),int(size-stride):int(size)]\n",
    "            \n",
    "            y = (y+1)%n_slices_vertical\n",
    "            if y == 0: x = x + 1\n",
    "            batch_index = batch_index + 1\n",
    "        \n",
    "  \n",
    "        #Máscara \n",
    "        \n",
    "        mask_per = x_per/(x_har+x_per+1e-16)\n",
    "        mask_har = x_har/(x_har+x_per+1e-16)\n",
    "\n",
    "        limiar_mask_per = mask_per.mean()*pct_limiar\n",
    "        limiar_mask_har = mask_har.mean()*pct_limiar\n",
    "        #print(\"limiar_per\",limiar_mask_per,\"limiar_har\",limiar_mask_har)\n",
    "\n",
    "        for i in range(int(x_mix.shape[0]-sobra_vertical)):\n",
    "            for j in range(int(x_mix.shape[1]-sobra_horizontal)):\n",
    "                if mask_per[i,j] < limiar_mask_per:\n",
    "                    mask_per[i,j] = 0\n",
    "                if mask_har[i,j] < limiar_mask_har:\n",
    "                    mask_har[i,j] = 0\n",
    "        \n",
    "                    \n",
    "        enxerto_vertical_per_mask = x_mix[int(x_mix.shape[0]-sobra_vertical-1),0:int(x_mix.shape[1]-sobra_horizontal)]*mask_per\n",
    "        enxerto_vertical_har_mask = x_mix[int(x_mix.shape[0]-sobra_vertical-1),0:int(x_mix.shape[1]-sobra_horizontal)]*mask_har\n",
    "        \n",
    "        x_mix_mask = x_mix[0:int(x_mix.shape[0]-sobra_vertical),0:int(x_mix.shape[1]-sobra_horizontal)]\n",
    "        \n",
    "        x_per_mask = x_mix_mask*mask_per\n",
    "        x_har_mask = x_mix_mask*mask_har\n",
    "        \n",
    "        #Return phase and cut slice\n",
    "        x_per_mask = x_per_mask*np.cos(phase)+1j*(x_per_mask*np.sin(phase))\n",
    "        x_har_mask = x_har_mask*np.cos(phase)+1j*(x_har_mask*np.sin(phase))\n",
    "        \n",
    "        enxerto_vertical_per_mask = enxerto_vertical_per_mask*np.cos(np.angle(enxerto_vertical))+1j*(enxerto_vertical_per_mask*np.sin(np.angle(enxerto_vertical)))\n",
    "        enxerto_vertical_har_mask = enxerto_vertical_har_mask*np.cos(np.angle(enxerto_vertical))+1j*(enxerto_vertical_har_mask*np.sin(np.angle(enxerto_vertical)))\n",
    "        \n",
    "        if sobra_vertical > 0:\n",
    "            x_per_mask = np.vstack((x_per_mask,enxerto_vertical_per_mask))\n",
    "            x_har_mask = np.vstack((x_har_mask,enxerto_vertical_har_mask))\n",
    "        \n",
    "        #IFFT\n",
    "\n",
    "        est_per = librosa.istft(x_per_mask, win_length = n_window, n_fft = n_fft, hop_length = hop_length, window = 'hamming', center=False)\n",
    "        est_har = librosa.istft(x_har_mask, win_length = n_window, n_fft = n_fft, hop_length = hop_length, window = 'hamming', center=False)\n",
    "    \n",
    "    #Para comparação, deve-se retirar a parte cortada\n",
    "    track = track[:,0:int(est_per.size)]\n",
    "    track_per = track_per[:,0:int(est_per.size)]\n",
    "    track_har = track_har[:,0:int(est_har.size)]\n",
    "    \n",
    "    return mask_per, mask_har, track, track_per, track_har, est_per, est_har"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4638f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualização de espectrogramas a partir das Stems diretamente\n",
    "music_index =  100\n",
    "stem = 3\n",
    "sr = 16000\n",
    "hop_length = 256\n",
    "n_window = 1024\n",
    "n_fft = 1024\n",
    "stem_espectrograma(music_index, stem, sr, hop_length, n_window, n_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a10707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualização de espectrogramas salvos\n",
    "track = 45\n",
    "print(\"Mistura\")\n",
    "mostra_espectrograma(0,track, 256, 1024,1024)\n",
    "print(\"Percussivo\")\n",
    "mostra_espectrograma(1,track, 256, 1024,1024)\n",
    "print(\"Harmônico\")\n",
    "mostra_espectrograma(2,track, 256, 1024,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação do dataset em espectrogramas\n",
    "# Rodar apenas se não tiver os espectrogramas salvos ainda\n",
    "hop_length = 256\n",
    "n_fft=1024\n",
    "n_window=1024\n",
    "prepara_dataset(hop_length, n_window, n_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cef055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo\n",
    "#Net model\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        \n",
    "        #Redução de dimensão\n",
    "        self.encoder = nn.Sequential(nn.Conv2d(1,64, stride = 1, padding =1, kernel_size=(3,3)),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.MaxPool2d(2, 2),\n",
    "                                     nn.Conv2d(64, 32, stride = 1, padding =1, kernel_size=(3,3)),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.MaxPool2d(2, 2),\n",
    "                                     nn.Conv2d(32, 16, stride = 1, padding =1, kernel_size=(3,3)),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.MaxPool2d(2, 2))\n",
    "    \n",
    "    \n",
    "        #Aumento de dimensao para parcela percussiva\n",
    "        self.decoder_per = nn.Sequential(nn.ConvTranspose2d(16, 16, stride = 1, padding =1, kernel_size=(3,3)),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.UpsamplingNearest2d(scale_factor=2),\n",
    "                                         nn.ConvTranspose2d(16, 32, stride = 1, padding =1, kernel_size=(3,3)),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.UpsamplingNearest2d(scale_factor=2),\n",
    "                                         nn.ConvTranspose2d(32, 64, stride = 1, padding =1, kernel_size=(3,3)),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.UpsamplingNearest2d(scale_factor=2),\n",
    "                                         nn.ConvTranspose2d(64, 1, stride = 1, padding =1, kernel_size=(3,3)),\n",
    "                                         nn.ReLU())\n",
    "     \n",
    "        \n",
    "        #Aumento de dimensao para parcela harmônica\n",
    "        self.decoder_har = nn.Sequential(nn.ConvTranspose2d(16, 16, stride = 1, padding =1, kernel_size=(3,3)),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.UpsamplingNearest2d(scale_factor=2),\n",
    "                                         nn.ConvTranspose2d(16, 32, stride = 1, padding =1, kernel_size=(3,3)),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.UpsamplingNearest2d(scale_factor=2),\n",
    "                                         nn.ConvTranspose2d(32, 64, stride = 1, padding =1, kernel_size=(3,3)),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.UpsamplingNearest2d(scale_factor=2),\n",
    "                                         nn.ConvTranspose2d(64, 1, stride = 1, padding =1, kernel_size=(3,3)),\n",
    "                                         nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)  \n",
    "        x_per = self.decoder_per(x)  \n",
    "        x_har = self.decoder_har(x)    \n",
    "        \n",
    "        return x_per, x_har"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feadad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametros\n",
    "n_tracks_train = 100\n",
    "n_tracks_validation = 50\n",
    "hop_length = 256\n",
    "n_fft=1024\n",
    "n_window=1024\n",
    "size=512\n",
    "stride=128\n",
    "batch_size = 5 #suportado pela GPU (Variável com as configurações)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26cb545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parametros \n",
    "num_epochs = 20\n",
    "learning_rate = 0.1\n",
    "n_batch_train = total_batches('train', n_tracks_train, batch_size, size, stride)\n",
    "print()\n",
    "n_batch_validation = total_batches('validation', n_tracks_validation, batch_size, size, stride)\n",
    "normalization = False #Normlização do Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c66c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gpu support\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    \n",
    "model = ConvAutoEncoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, '\\t\\t\\t', param.shape,'\\t\\t\\t',param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset\n",
    "esp_sizes_train, esp_hops_train, dataset_mix_train, dataset_per_train, dataset_har_train = carrega_dataset('train', n_tracks_train, hop_length, n_window, n_fft)\n",
    "esp_sizes_validation, esp_hops_validation, dataset_mix_validation, dataset_per_validation, dataset_har_validation = carrega_dataset('validation', n_tracks_validation, hop_length, n_window, n_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd96bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo do zero\n",
    "loaded = 0\n",
    "normalization = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071cd84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model and error, if saved\n",
    "loaded = 1\n",
    "model.load_state_dict(torch.load('C:/Users/gusta/Modelo_1.2_50epoch_rate=0.1'))\n",
    "\n",
    "epoch_loss_train = {}\n",
    "epoch_loss_validation = {}\n",
    "\n",
    "epoch_loss_train_df = pd.read_csv('C:/Users/gusta/Loss_train_1.2_50epoch_rate=0.1')\n",
    "epoch_loss_validation_df = pd.read_csv('C:/Users/gusta/Loss_validation_1.2_50epoch_rate=0.1')\n",
    " \n",
    "epoch_loss_train['total'] = epoch_loss_train_df['total'].values.tolist()\n",
    "epoch_loss_train['percursive'] = epoch_loss_train_df['percursive'].values.tolist()\n",
    "epoch_loss_train['harmonic'] = epoch_loss_train_df['percursive'].values.tolist()\n",
    "\n",
    "epoch_loss_validation['total'] = epoch_loss_validation_df['total'].values.tolist()\n",
    "epoch_loss_validation['percursive'] = epoch_loss_validation_df['percursive'].values.tolist()\n",
    "epoch_loss_validation['harmonic'] = epoch_loss_validation_df['percursive'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "\n",
    "#Caso comece do zero\n",
    "if loaded == 0:\n",
    "    epoch_loss_train = {}\n",
    "    epoch_loss_train['total'] = []\n",
    "    epoch_loss_train['percursive'] = []\n",
    "    epoch_loss_train['harmonic'] = []\n",
    "    \n",
    "    epoch_loss_validation = {}\n",
    "    epoch_loss_validation['total'] = []\n",
    "    epoch_loss_validation['percursive'] = []\n",
    "    epoch_loss_validation['harmonic'] = []\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(int(num_epochs)):\n",
    "    model.train()\n",
    "    selection_train = np.random.choice(n_tracks_train, size=n_tracks_train, replace=False)\n",
    "    \n",
    "    running_loss = 0\n",
    "    running_loss_per = 0\n",
    "    running_loss_har = 0\n",
    "    \n",
    "    current_track = -1\n",
    "    slices_left = 0\n",
    "    \n",
    "    print()\n",
    "    print(\"Train \")\n",
    "    for i in range(int(n_batch_train)):\n",
    "        #Obtém Batch\n",
    "        current_track, slices_left, x_mix, x_per, x_har = fornece_batches(esp_sizes_train, esp_hops_train, dataset_mix_train, dataset_per_train, dataset_har_train, batch_size, current_track, slices_left, selection_train, size, stride, n_fft, normalization = normalization)\n",
    "\n",
    "        #Treino na GPU\n",
    "        #x_mix = x_mix.to(device) \n",
    "        x_per = x_per.to(device)\n",
    "        x_har = x_har.to(device)\n",
    "        \n",
    "        # Forward pass and error\n",
    "        optimizer.zero_grad()\n",
    "        #x_per_est, x_har_est = model(x_mix)\n",
    "        x_per_est, x_har_est = model(x_per + x_har)\n",
    "        loss_per = criterion(x_per_est, x_per)\n",
    "        loss_har = criterion(x_har_est, x_har)\n",
    "        loss = criterion(x_har_est, x_har)+criterion(x_per_est, x_per)\n",
    "        \n",
    "        running_loss = running_loss + loss.item()\n",
    "        running_loss_per = running_loss_per + loss_per.item()\n",
    "        running_loss_har = running_loss_har + loss_har.item()\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print (f'Train mode : Current Track [{current_track}], Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_batch_train}], Duration Time: {time.time()-start_time:.4f}s, Loss: {loss.item():.4f}, Loss Per: {loss_per.item():.4f}, Loss Har: {loss_har.item():.4f}')\n",
    "\n",
    "    \n",
    "    epoch_loss_train['total'].append(running_loss/n_batch_train)\n",
    "    epoch_loss_train['percursive'].append(running_loss_per/n_batch_train)\n",
    "    epoch_loss_train['harmonic'].append(running_loss_har/n_batch_train)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        selection_validation = np.random.choice(n_tracks_validation, size=n_tracks_validation, replace=False)\n",
    "\n",
    "        running_loss = 0\n",
    "        running_loss_per = 0\n",
    "        running_loss_har = 0\n",
    "\n",
    "        current_track = -1\n",
    "        slices_left = 0\n",
    "\n",
    "        print()\n",
    "        print(\"Validation \")\n",
    "\n",
    "        for i in range(int(n_batch_validation)):\n",
    "            #Obtém Batch\n",
    "            current_track, slices_left, x_mix, x_per, x_har = fornece_batches(esp_sizes_validation, esp_hops_validation, dataset_mix_validation, dataset_per_validation, dataset_har_validation, batch_size, current_track, slices_left, selection_validation, size, stride, n_fft, normalization = normalization)\n",
    "\n",
    "            #Validação na GPU\n",
    "            #x_mix = x_mix.to(device)\n",
    "            x_per = x_per.to(device)\n",
    "            x_har = x_har.to(device)\n",
    "\n",
    "            #Model output and error\n",
    "            #x_per_est, x_har_est = model(x_mix)\n",
    "            x_per_est, x_har_est = model(x_per + x_har)\n",
    "            loss_per = criterion(x_per_est, x_per)\n",
    "            loss_har = criterion(x_har_est, x_har)\n",
    "            loss = criterion(x_har_est, x_har)+criterion(x_per_est, x_per)\n",
    "\n",
    "            running_loss = running_loss + loss.item()\n",
    "            running_loss_per = running_loss_per + loss_per.item()\n",
    "            running_loss_har = running_loss_har + loss_har.item()\n",
    "\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                print (f'Validation mode : Current Track [{current_track}], Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_batch_validation}] , Duration Time: {time.time()-start_time:.4f}s, Loss: {loss.item():.4f}, Loss Per: {loss_per.item():.4f}, Loss Har: {loss_har.item():.4f}')\n",
    "\n",
    "        epoch_loss_validation['total'].append(running_loss/n_batch_validation)\n",
    "        epoch_loss_validation['percursive'].append(running_loss_per/n_batch_validation)\n",
    "        epoch_loss_validation['harmonic'].append(running_loss_har/n_batch_validation)\n",
    "\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    \n",
    "plt.figure(1)\n",
    "plt.title('Mean Loss per epoch')\n",
    "plt.plot(epoch_loss_train['total'])\n",
    "plt.plot(epoch_loss_train['percursive'])\n",
    "plt.plot(epoch_loss_train['harmonic'])\n",
    "plt.plot(epoch_loss_validation['total'])\n",
    "plt.plot(epoch_loss_validation['percursive'])\n",
    "plt.plot(epoch_loss_validation['harmonic'])\n",
    "plt.legend(['Train Total','Train Percursiva','Train Harmonica','Val Total','Val Percursiva','Val Harmonica'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61410b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(repoch_loss_train['total'])\n",
    "plt.plot(epoch_loss_validation['total'])\n",
    "\n",
    "plt.legend(['Treino','Validação'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62561b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIR para diminuir a variância da validação\n",
    "for i in range(50):\n",
    "    if i>2 and i<47:\n",
    "        epoch_loss_validation['total'][i] = epoch_loss_validation['total'][i]*(4/16)+epoch_loss_validation['total'][i-1]*(3/16)+epoch_loss_validation['total'][i-2]*(2/16)+epoch_loss_validation['total'][i-3]*(1/16)+epoch_loss_validation['total'][i+1]*(3/16)+epoch_loss_validation['total'][i+2]*(2/16)+epoch_loss_validation['total'][i+3]*(1/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e2b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificar Modelo\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Model and error \n",
    "torch.save(model.state_dict(), 'C:/Users/gusta/Modelo_1.1_20epoch_rate=0.1')\n",
    "\n",
    "epoch_loss_train_df = pd.DataFrame(epoch_loss_train)\n",
    "epoch_loss_train_df.to_csv('C:/Users/gusta/Loss_train_1.1_20epoch_rate=0.1', index = False)\n",
    "\n",
    "epoch_loss_validation_df = pd.DataFrame(epoch_loss_validation)\n",
    "epoch_loss_validation_df.to_csv('C:/Users/gusta/Loss_validation_1.1_20epoch_rate=0.1', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b765cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean GPU memory\n",
    "optimizer.zero_grad()\n",
    "del x_mix\n",
    "del x_per\n",
    "del x_har\n",
    "del x_har_est\n",
    "del x_per_est\n",
    "del loss\n",
    "del optimizer\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print('Memory Usage:')\n",
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,5), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,5), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0471857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "index = 139\n",
    "normalization = True\n",
    "pct_limiar = 0\n",
    "mask_per, mask_har, track, track_per, track_har, est_per, est_har = apply_model(index, model, batch_size, n_window, n_fft, hop_length, size, stride, normalization = normalization, pct_limiar = pct_limiar)\n",
    "\n",
    "print(\"Track: \"+str(index)+ \" Stem: Mixture\")\n",
    "ipd.Audio(track, rate = 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f336cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask_per,aspect='auto',vmin=0, origin = 'lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d53c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask_har, aspect='auto',vmin=0, origin = 'lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a85123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Track: \"+str(index)+\" Stem: Percursive\")\n",
    "ipd.Audio(track_per, rate = 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ccfedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Track: \"+str(index)+\" Stem: Harmonic\")\n",
    "ipd.Audio(track_har, rate = 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5910ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model output with Mask type 1: \"+str(index)+\" Stem: Percursive\")\n",
    "ipd.Audio(est_per, rate = 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5636a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model output with Mask type 1: \"+str(index)+\" Stem: Harmonic\")\n",
    "ipd.Audio(est_har, rate = 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775fdf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation\n",
    "\n",
    "normalization = True\n",
    "n_tracks_train = 100\n",
    "n_tracks_validation = 50\n",
    "hop_length = 256\n",
    "n_fft=1024\n",
    "n_window=1024\n",
    "size=512\n",
    "stride=128\n",
    "pct_limiar = 0\n",
    "\n",
    "metric_train = {}\n",
    "metric_train['Per SDR'] = []\n",
    "metric_train['Per SAR'] = []\n",
    "metric_train['Per SIR'] = []\n",
    "metric_train['Har SDR'] = []\n",
    "metric_train['Har SAR'] = []\n",
    "metric_train['Har SIR'] = []\n",
    "\n",
    "\n",
    "metric_validation = {}\n",
    "metric_validation['Per SDR'] = []\n",
    "metric_validation['Per SAR'] = []\n",
    "metric_validation['Per SIR'] = []\n",
    "metric_validation['Har SDR'] = []\n",
    "metric_validation['Har SAR'] = []\n",
    "metric_validation['Har SIR'] = []\n",
    "\n",
    "sdr = 0\n",
    "sar = 0\n",
    "sir = 0\n",
    "isr = 0\n",
    "perm = 0\n",
    "running_sdr_per =0\n",
    "running_sar_per =0\n",
    "running_sir_per =0\n",
    "running_sdr_har =0\n",
    "running_sar_har =0\n",
    "running_sir_har =0\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "print()\n",
    "print(\"Train\")\n",
    "\n",
    "for i in range(int(n_tracks_train)):\n",
    "\n",
    "    mask_per, mask_har, track, track_per, track_har, est_per, est_har = apply_model(i, model, batch_size, n_window, n_fft, hop_length, size, stride, normalization = normalization, pct_limiar = pct_limiar)\n",
    "    #reference = np.array([track_per.reshape(-1,1),track_har.reshape(-1,1)]) #harmonic reference is just harmonic target\n",
    "    reference = np.array([track_per.reshape(-1,1),(track-track_per).reshape(-1,1)]) #harmonic reference is everything else drums\n",
    "    estimated = np.array([est_per.reshape(-1,1),est_har.reshape(-1,1)])\n",
    "\n",
    "    sdr,isr,sir,sar,perm = museval.metrics.bss_eval(reference, estimated, window = np.inf)\n",
    "    running_sdr_per = running_sdr_per + 10**(sdr[0].mean()/10)\n",
    "    running_sar_per = running_sar_per + 10**(sar[0].mean()/10)\n",
    "    running_sir_per = running_sir_per + 10**(sir[0].mean()/10) \n",
    "    running_sdr_har = running_sdr_har + 10**(sdr[1].mean()/10)\n",
    "    running_sar_har = running_sar_har + 10**(sar[1].mean()/10)\n",
    "    running_sir_har = running_sir_har + 10**(sir[1].mean()/10) \n",
    "    \n",
    "    if (i+1) % 10 == 0:\n",
    "        print (f'Train mode : Current Track [{i}], Duration Time: {time.time()-start_time:.4f}s')\n",
    "\n",
    "metric_train['Per SDR'].append(10*np.log10(running_sdr_per/n_tracks_train+1e-16))\n",
    "metric_train['Per SAR'].append(10*np.log10(running_sar_per/n_tracks_train+1e-16))\n",
    "metric_train['Per SIR'].append(10*np.log10(running_sir_per/n_tracks_train+1e-16)) \n",
    "\n",
    "metric_train['Har SDR'].append(10*np.log10(running_sdr_har/n_tracks_train+1e-16))\n",
    "metric_train['Har SAR'].append(10*np.log10(running_sar_har/n_tracks_train+1e-16))\n",
    "metric_train['Har SIR'].append(10*np.log10(running_sir_har/n_tracks_train+1e-16)) \n",
    "\n",
    "print(\"----------------------------------------------------------\")\n",
    "\n",
    "running_sdr_per =0\n",
    "running_sar_per =0\n",
    "running_sir_per =0\n",
    "running_sdr_har =0\n",
    "running_sar_har =0\n",
    "running_sir_har =0\n",
    "\n",
    "print()\n",
    "print(\"Validation\")\n",
    "for i in range(int(n_tracks_validation)):\n",
    "\n",
    "    mask_per, mask_har, track, track_per, track_har, est_per, est_har = apply_model(i+100, model, batch_size, n_window, n_fft, hop_length, size, stride, normalization = normalization, pct_limiar = pct_limiar)\n",
    "    #reference = np.array([track_per.reshape(-1,1),track_har.reshape(-1,1)]) #harmonic reference is just harmonic target\n",
    "    reference = np.array([track_per.reshape(-1,1),(track-track_per).reshape(-1,1)]) #harmonic reference is everything else drums\n",
    "    estimated = np.array([est_per.reshape(-1,1),est_har.reshape(-1,1)])\n",
    "    \n",
    "    sdr,isr,sir,sar,perm = museval.metrics.bss_eval(reference, estimated, window = np.inf)\n",
    "    running_sdr_per = running_sdr_per + 10**(sdr[0].mean()/10)\n",
    "    running_sar_per = running_sar_per + 10**(sar[0].mean()/10)\n",
    "    running_sir_per = running_sir_per + 10**(sir[0].mean()/10) \n",
    "    running_sdr_har = running_sdr_har + 10**(sdr[1].mean()/10)\n",
    "    running_sar_har = running_sar_har + 10**(sar[1].mean()/10)\n",
    "    running_sir_har = running_sir_har + 10**(sir[1].mean()/10) \n",
    "    \n",
    "\n",
    "    if (i+1) % 10 == 0:\n",
    "        print (f'Validation mode : Current Track [{i}], Duration Time: {time.time()-start_time:.4f}s')\n",
    "\n",
    "metric_validation['Per SDR'].append(10*np.log10(running_sdr_per/n_tracks_validation+1e-16))\n",
    "metric_validation['Per SAR'].append(10*np.log10(running_sar_per/n_tracks_validation+1e-16))\n",
    "metric_validation['Per SIR'].append(10*np.log10(running_sir_per/n_tracks_validation+1e-16)) \n",
    "\n",
    "metric_validation['Har SDR'].append(10*np.log10(running_sdr_har/n_tracks_validation+1e-16))\n",
    "metric_validation['Har SAR'].append(10*np.log10(running_sar_har/n_tracks_validation+1e-16))\n",
    "metric_validation['Har SIR'].append(10*np.log10(running_sir_har/n_tracks_validation+1e-16)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ecc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save metrics\n",
    "metric_train_df = pd.DataFrame(metric_train)\n",
    "metric_train_df.to_csv('C:/Users/gusta/Loss_train_1.1_50epoch_limiar=corr', index = False)\n",
    "\n",
    "metric_validation_df = pd.DataFrame(metric_validation)\n",
    "metric_validation_df.to_csv('C:/Users/gusta/Loss_validation_1.1_50epoch_limiar=corr', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a93ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load metrics\n",
    "metric_train = {}\n",
    "metric_validation = {}\n",
    "\n",
    "metric_train_df = pd.read_csv('C:/Users/gusta/Loss_train_1.1_50epoch_limiar=100')\n",
    "metric_validation_df = pd.read_csv('C:/Users/gusta/Loss_validation_1.1_50epoch_limiar=100')\n",
    " \n",
    "metric_train['Per SDR'] = metric_train_df['Per SDR'].values.tolist()\n",
    "metric_train['Per SAR'] = metric_train_df['Per SAR'].values.tolist()\n",
    "metric_train['Har SDR'] = metric_train_df['Har SDR'].values.tolist()\n",
    "metric_train['Har SAR'] = metric_train_df['Har SAR'].values.tolist()\n",
    "\n",
    "metric_validation['Per SDR'] = metric_validation_df['Per SDR'].values.tolist()\n",
    "metric_validation['Per SAR'] = metric_validation_df['Per SAR'].values.tolist()\n",
    "metric_validation['Har SDR'] = metric_validation_df['Har SDR'].values.tolist()\n",
    "metric_validation['Har SAR'] = metric_validation_df['Har SAR'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show metrics\n",
    "metric_validation = {}\n",
    "metric_validation_df = pd.read_csv('C:/Users/gusta/Loss_validation_1.1_50epoch_limiar=0')\n",
    "metric_validation_norm_df = pd.read_csv('C:/Users/gusta/Loss_validation_1.2_50epoch_limiar=0')\n",
    "data = [[float(\"%.2f\" % metric_validation_df['Per SDR'][0]),float(\"%.2f\" % metric_validation_norm_df['Per SDR'][0]),2.51],[float(\"%.2f\" % metric_validation_df['Per SAR'][0]),float(\"%.2f\" % metric_validation_norm_df['Per SAR'][0]),3.53],[float(\"%.2f\" % metric_validation_df['Per SIR'][0]),float(\"%.2f\" % metric_validation_norm_df['Per SIR'][0]),14.15]]\n",
    "\n",
    "X = np.arange(3)\n",
    "fig = plt.figure(1)\n",
    "ax = plt.bar(X + 0.00, data[0], color = 'b', width = 0.25)\n",
    "ax1 = plt.bar(X + 0.25, data[1], color = 'g', width = 0.25,tick_label = ['Sem Norm','Norm','Ref'])\n",
    "ax2 = plt.bar(X + 0.50, data[2], color = 'r', width = 0.25)\n",
    "plt.legend(['SDR','SAR','SIR'])\n",
    "plt.bar_label(ax)\n",
    "plt.bar_label(ax1)\n",
    "plt.bar_label(ax2)\n",
    "\n",
    "metric_validation = {}\n",
    "metric_validation_df = pd.read_csv('C:/Users/gusta/Loss_validation_1.1_50epoch_limiar=0')\n",
    "metric_validation_norm_df = pd.read_csv('C:/Users/gusta/Loss_validation_1.2_50epoch_limiar=0')\n",
    "data = [[float(\"%.2f\" % metric_validation_df['Har SDR'][0]),float(\"%.2f\" % metric_validation_norm_df['Har SDR'][0]),9.61],[float(\"%.2f\" % metric_validation_df['Har SAR'][0]),float(\"%.2f\" % metric_validation_norm_df['Har SAR'][0]),10.20],[float(\"%.2f\" % metric_validation_df['Har SIR'][0]),float(\"%.2f\" % metric_validation_norm_df['Har SIR'][0]),20.27]]\n",
    "\n",
    "X = np.arange(3)\n",
    "fig = plt.figure(2)\n",
    "ax = plt.bar(X + 0.00, data[0], color = 'b', width = 0.25)\n",
    "ax1 = plt.bar(X + 0.25, data[1], color = 'g', width = 0.25,tick_label = ['Sem Norm','Norm','Ref'])\n",
    "ax2 = plt.bar(X + 0.50, data[2], color = 'r', width = 0.25)\n",
    "plt.legend(['SDR','SAR','SIR'])\n",
    "plt.bar_label(ax)\n",
    "plt.bar_label(ax1)\n",
    "plt.bar_label(ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(4)\n",
    "plt.title('Percusive')\n",
    "ax = plt.bar('SDR',metric_validation['Per SDR'])\n",
    "ax1 = plt.bar('SAR',metric_validation['Per SAR'])\n",
    "ax2 = plt.bar('SIR',metric_validation['Per SIR'])\n",
    "plt.legend(['SDR','SAR','SIR'])\n",
    "plt.bar_label(ax)\n",
    "plt.bar_label(ax1)\n",
    "plt.bar_label(ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b27562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
